---
title: "Notes on Machine Learning"
date: 2019-01-01
tags: [reviews, machine learning]
excerpt: "reviews, machine learning"
mathjax: "true"
---

## Concepts

### Statistical Learning vs Machine Learning

 Different goals: 
 
 Statistical Learning: inference and generalization
 
 Machine Learning: maxmize prediction accuracy
 
 > Machine learning is the science of getting computers to act without being explicitly programmed. - Andrew Ng
 
 > Field of study that gives computers the ability to learn without being explicitly programmed. -Arthur Samuel, IBM, 1959
 
### Model

  Common form: $$y = f(x) + \epsilon $$; Interested in estimating functional form of f(x)
  
  Approaches: parametric; non=parametric estimation
 
### Selecting and fitting a model

### Flexible models vs. inflexible models 
 
  `Flexibility` is a measure of how much a fitted model can vary with a given data.
  
  * Example of flexible models (high variance, low bias): splines, 10 degree curve, Support Vector Machines
  
  * Example of inflexible models (low variance, high bias): linear regression

 * When a inflexible model is preferred:
 
   1. interpretablility: understand how the explanatory variables affect the response variable
      
   2. a small sample size (small n) and large number of predictors (large p)
   
   3. the variance of the error terms, i.e. $$sigma^{2}=Var(\epsilon)$$, is extremely high
 
 * When a flexible model is preferred:
   
   1. a large sample size (large n) and small number of predictors (small p)
   
   2. relationship between the predictors and response is highly non-linear
   
A flexible model will cause you to fit too much of the noise in the problem (when variance of the error terms is high).

[Quora answer](https://www.quora.com/What-are-flexible-statistical-learning-methods)


### Bias / Variance

#### Error

Error = Irreducible Error + $$ Bias^2 $$ + Variance

#### Bias-variance tradeoff

Metrics for evaluating models

"No Free Lunch Theorem"


Measurement (Precision / Recall / AUC / Accuracy)


### AUC
  
[Blog](https://arogozhnikov.github.io/2015/10/05/roc-curve.html)

### TPR/FPR/TNR/FNR

Optimization Methods

1. Netwon's method

2. Quasi-Newton methods (ex. LBFGS)

3. Gradient Descent

Underfitting & overfitting

Hyper-parameter tuning 

### Resampling Methods

* Essential to test and evaluate statistical models

* Repeatedly draw from your original sample to obtain additional information about the model

#### Train and testing set

* training set is used for model construction (can be reused many times to build different models)

* test set is used to evaluate the performance of the final model (can be used only once)

#### Validation

* Intuition: fit the model and evaluate it many times on the same data

* Method: split half of the training data as validation set

* Drawback: validation estimates of the test error rates can be highly variable depending on which observations are sampled into the training and validation sets (ex. outliers in the validation sets)

#### Cross-validation
_for evaluating a model’s performance relative to other models_

1. LOOCV: leave-one-out cross validation

only remove one observation for the validation set, and keep all remaining observations in the training set

$$ CV_N = {\frac{1}{N}}{\sum_{i=1}^{N}}MSE_i $$

* Pro: unbiased compared to naive 1-stage validation approach

* Pro: highly flexible and works with any kind of predictive modeling

* Cro: high variance because the N "training sets" are so similar to one another; 

* Cro: computationally expensive

2. K-fold cross-validation: divides the observations into K folds of approximately equal size

Typical K= 5 or 10; Random sampling without replacement

$$ CV_K = \frac{1}{K}\sum_{i=1}^{K}MSE_i $$

* Pro: less variance in test error estimate compared with LOOCV

* Cro: leads to a slight increase in usually bias

3. Other validations

* Stratified cross-validation

* Repeated cross-validation 

* Cross-validation with time series data

#### Bootstrapping
_non-parametric measure of the accuracy of a parameter estimate or method_

Purpose: quantify uncertainty associated with some estimator

Loss

1. MSE/Sum of squared errors

2. Cross-entropy

3. Kullback-Leibler (KL) divergence

4. Gini impurity for decision tree

5. Information gain

### Estimators of erros

Mallows’s Cp

[Wiki](https://en.wikipedia.org/wiki/Mallows%27s_Cp)

AIC

BIC

Data Preparation

1. Cleaning/Pre-processing

2. Missing data

3. Unbalanced data

[How to fix an Unbalanced Dataset@KDnuggets](https://www.kdnuggets.com/2019/05/fix-unbalanced-dataset.html)

4. Feature engineering

5. Feature selection

6. Effective sample size

7. Time correaltions

* backward

* forward

* subset

* L1

* L2

* PCA

Ensemble learning

1. Bagging

2. Boosting (xgboost)

Estimation Methods

1. MLE / MAP

2. Expectation-Maxmization algorithm

Occam's Razor
  
  "the simplest solution is most likely the right one"
  
  "when presented with competing hypotheses that make the same predictions, one should select the solution with the fewest assumptions"
  
[Wikipedia](https://en.wikipedia.org/wiki/Occam%27s_razor)

### Building ML Model

<img src="/images/ML/ML.jfif" class="img-responsive" alt=""> 


## Supervised Machine Learning

### Regression

<details>
<summary>Linear Model</summary>
<br>
  
Two assumptions: 1. Linear; 2. Additive (unit changes)
  
<br>

</details>

OLS (ordinary least squares)

Linear model selection

1. Linearity

2. Correlation vs Causality

3. Multicollinearity

4. Interaction variables (ex. x1*x2)

5. Normality

6. Homoscedasticity

7. Monotonic relationship

### Regularization 

1. Ridge

2. Lasso 

3. Elastic-net

Bayesian linear regression

Local regression

Generalized additive model

Logistic regression

### Classification

Purpose: categorize all the variables that form the output

Softmax classifier

K-nearest neighbors

Naïve bayes

Support vector machine

### Regression vs Classification

 Different `y`: 
 
Regression: response is quantitative (continuous)
 
Classification: response is qualitative (binary/multinomial)

### Tree-based Methods

Classification trees

Boosted regression trees

Random forests

GBDT: Gradient Boosting Decision Tree

GBDT loss fucntion

## Unsupervised Machine Learning

### Exploratoty data analysis

### Visualization 

1. Histograms

2. Boxplot

3. t-SNE

### Factor analysis

### Principal component analysis

### Discriminant analysis

### Clustering

1. Distance

2. Clusterability

3. Kmeans

4. Hard partitioning

5. Soft partitioning

6. Hierarchical clustering

7. Gaussian mixture models

### Association rule mining


## Natural Language Processing

### Text Mining

Text mining is the process of examining large collections of text and converting the unstructured text data into structured data for further analysis like visualization and model building.

### Word Embedding

### BERT

### Pre-processing

### Sentiment Analysis

### Topic Models

1. LDA/Gibbs 

2. Structural

3. Non-negative factorization

Hidden Markov Chain

### Knowledge graph

Graph theory + Network Science + NLP ---> Better search result with multiple sources

Python tutorial: [how-to-build-knowledge-graph-text-using-spacy](https://www.analyticsvidhya.com/blog/2019/10/how-to-build-knowledge-graph-text-using-spacy/)

Companies: 

[HopHR Knowledge Graph Data Scientist](https://www.linkedin.com/jobs/view/principal-knowledge-graph-data-scientist-md-maryland-at-hophr-1551159673/)

[Big CLoud Data Scientist](https://www.linkedin.com/jobs/view/data-scientist-knowledge-graphs-at-big-cloud-1582125196/?originalSubdomain=sg)

Papers:

[A Review of Relational Machine Learning for Knowledge Graphs](https://arxiv.org/pdf/1503.00759.pdf)

### Application: Chatbox

## Reinforcement Learning

Purpose: max the performance of the machine in a way that helps it to grow

GAN

## Deep Learning

Single layer and multi-layer perceptron neural nets

Convolutional Neural Network

Recursive Neural Network

Self-organizing maps

Dropout

Batch Normalization

Back propagation

Keras

Tensorflow

## Recommendation Systems

Collaborative Filtering

## Information Retrieval

Search/Display

Ads retrieval and recommendation

Spam-traffic and click-farm detection

Infrastructure development

Traffic and revenue prediction

Ads pricing

Audience expansion

## Links to online resources

[An Introduction to Statistical Learning](https://faculty.marshall.usc.edu/gareth-james/ISL/ISLR%20Seventh%20Printing.pdf)

[The Elements of Statistical Learning (2nd edition)](https://web.stanford.edu/~hastie/ElemStatLearn/printings/ESLII_print12.pdf)

[CS231n Convolutional Neural Networks for Visual Recognition](http://cs231n.github.io/)


<details>
<summary> 
  
</summary>
<br>
  
<br>
</details>
