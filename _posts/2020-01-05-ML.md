---
title: "Studying Notes on Machine Learning"
date: 2019-01-01
tags: [reviews, machine learning]
excerpt: "reviews, machine learning"
mathjax: "true"
---

## Concepts

### Statistical Learning vs Machine Learning

 Different goals: 
 
 Statistical Learning: inference and generalization
 
 Machine Learning: maxmize prediction accuracy
 
 > Machine learning is the science of getting computers to act without being explicitly programmed. - Andrew Ng
 
 > Field of study that gives computers the ability to learn without being explicitly programmed. -Arthur Samuel, IBM, 1959
 
### Model

  Common form: $$y = f(x) + \epsilon $$; Interested in estimating functional form of f(x)
  
  Approaches: parametric; non=parametric estimation
 
### Selecting and fitting a model

### Flexible models vs. inflexible models 
 
  `Flexibility` is a measure of how much a fitted model can vary with a given train data.
  
  * Example of flexible models (high variance, low bias): splines, 10 degree curve, Support Vector Machines
  
  * Example of inflexible models (low variance, high bias): linear regression

 * When a inflexible model is preferred:
 
   1. interpretablility: understand how the explanatory variables affect the response variable
   
   2. prediction: flexible models tend to overfit
   
   3. a large sample size (large n) and small number of predictors (small p)
   
   4. a small sample size (small n) and large number of predictors (large p)
 
 * When a flexible model is preferred:
 
   1. variance of the error terms, i.e. σ2 = Var(ε), is extremely high
   
   2. find the nonlinear effect
   
   3. better fit the train data
   
A flexible model will cause you to fit too much of the noise in the problem (when variance of the error terms is high).

[Quora answer](https://www.quora.com/What-are-flexible-statistical-learning-methods)


Bias / Variance

Metrics for evaluating models

"No Free Lunch Theorem"

Bias-variance tradeoff

Measurement (Precision / Recall / AUC / Accuracy)


### AUC
  
[Blog](https://arogozhnikov.github.io/2015/10/05/roc-curve.html)

### TPR/FPR/TNR/FNR

Optimization Methods

1. Netwon's method

2. Quasi-Newton methods (ex. LBFGS)

3. Gradient Descent

Underfitting & overfitting

Hyper-parameter tuning 

Resampling Methods

1. Cross-validation

2. Bootstrapping

Loss

1. MSE/Sum of squared errors

2. Cross-entropy

3. Kullback-Leibler (KL) divergence

4. Gini impurity for decision tree

5. Information gain


Data Preparation

1. Cleaning/Pre-processing

2. Missing data

3. Unbalanced data

4. Feature engineering

5. Feature selection

6. Effective sample size

7. Time correaltions

* backward

* forward

* subset

* L1

* L2

* PCA

Ensemble learning

1. Bagging

2. Boosting (xgboost)

Estimation Methods

1. MLE / MAP

2. Expectation-Maxmization algorithm

<details>
<summary>Occam's Razor</summary>
<br>
  
  "the simplest solution is most likely the right one"
  
  "when presented with competing hypotheses that make the same predictions, one should select the solution with the fewest assumptions"
  
[Wikipedia](https://en.wikipedia.org/wiki/Occam%27s_razor)

<br>

</details>

## Supervised Machine Learning

### Regression

<details>
<summary>Linear Model</summary>
<br>
  
Two assumptions: 1. Linear; 2. Additive (unit changes)
  
<br>

</details>

OLS (ordinary least squares)

Linear model selection

1. Linearity

2. Correlation vs Causality

3. Multicollinearity

4. Interaction variables (ex. x1*x2)

5. Normality

6. Homoscedasticity

7. Monotonic relationship

### Regularization 

1. Ridge

2. Lasso 

3. Elastic-net

Bayesian linear regression

Local regression

Generalized additive model

Logistic regression

### Classification

Purpose: categorize all the variables that form the output

Softmax classifier

K-nearest neighbors

Naïve bayes

Support vector machine

### Regression vs Classification
 Different `y`: 
 
Regression: response is quantitative (continuous)
 
Classification: response is qualitative (binary/multinomial)

### Tree-based Methods

Classification trees

Boosted regression trees

Random forests

GBDT: Gradient Boosting Decision Tree

GBDT loss fucntion

## Unsupervised Machine Learning

### Exploratoty data analysis

### Visualization 

1. Histograms

2. Boxplot

3. t-SNE

### Factor analysis

### Principal component analysis

### Discriminant analysis

### Clustering

1. Distance

2. Clusterability

3. Kmeans

4. Hard partitioning

5. Soft partitioning

6. Hierarchical clustering

7. Gaussian mixture models

### Association rule mining


## Natural Language Processing

### Text Mining

Text mining is the process of examining large collections of text and converting the unstructured text data into structured data for further analysis like visualization and model building.

### Word Embedding

### BERT

### Pre-processing

### Sentiment Analysis

### Topic Models

1. LDA/Gibbs 

2. Structural

3. Non-negative factorization

Hidden Markov Chain

### Knowledge graph

Graph theory + Network Science + NLP ---> Better search result with multiple sources

Python tutorial: [how-to-build-knowledge-graph-text-using-spacy](https://www.analyticsvidhya.com/blog/2019/10/how-to-build-knowledge-graph-text-using-spacy/)

Companies: 

[HopHR Knowledge Graph Data Scientist](https://www.linkedin.com/jobs/view/principal-knowledge-graph-data-scientist-md-maryland-at-hophr-1551159673/)

[Big CLoud Data Scientist](https://www.linkedin.com/jobs/view/data-scientist-knowledge-graphs-at-big-cloud-1582125196/?originalSubdomain=sg)

Papers:

[A Review of Relational Machine Learning for Knowledge Graphs](https://arxiv.org/pdf/1503.00759.pdf)

### Application: Chatbox

## Reinforcement Learning

Purpose: max the performance of the machine in a way that helps it to grow

GAN

## Deep Learning

Single layer and multi-layer perceptron neural nets

Convolutional Neural Network

Recursive Neural Network

Self-organizing maps

Dropout

Batch Normalization

Back propagation

Keras

Tensorflow

## Recommendation Systems

Collaborative Filtering

## Information Retrieval

Search/Display

Ads retrieval and recommendation

Spam-traffic and click-farm detection

Infrastructure development

Traffic and revenue prediction

Ads pricing

Audience expansion

## Links to online resources

[An Introduction to Statistical Learning](https://faculty.marshall.usc.edu/gareth-james/ISL/ISLR%20Seventh%20Printing.pdf)

[The Elements of Statistical Learning (2nd edition)](https://web.stanford.edu/~hastie/ElemStatLearn/printings/ESLII_print12.pdf)

[CS231n Convolutional Neural Networks for Visual Recognition](http://cs231n.github.io/)


<details>
<summary> 
  
</summary>
<br>
  
<br>
</details>
